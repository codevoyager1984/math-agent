#!/usr/bin/env python3
"""
ChromaDB å‘é‡å¯¼å‡ºè„šæœ¬
ä» ChromaDB ä¸­æ‹‰å–æ‰€æœ‰å‘é‡æ•°æ®ï¼ŒåŒ…æ‹¬æ–‡æ¡£å†…å®¹ã€å…ƒæ•°æ®ç­‰ä¿¡æ¯ï¼ˆä¸åŒ…å«å‘é‡åµŒå…¥ï¼‰
æ”¯æŒå®šæ—¶å¯¼å‡ºæ¨¡å¼ï¼Œæ¯10åˆ†é’Ÿè‡ªåŠ¨å¯¼å‡ºæ•°æ®åˆ°JSONæ–‡ä»¶
"""
import asyncio
import sys
import os
import argparse
import json
import signal
import time
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append('.')

import chromadb
from loguru import logger


class VectorExporter:
    """å‘é‡å¯¼å‡ºå™¨"""
    
    def __init__(self, host: str = "localhost", port: int = 8000):
        """
        åˆå§‹åŒ–å‘é‡å¯¼å‡ºå™¨
        
        Args:
            host: ChromaDB æœåŠ¡å™¨åœ°å€
            port: ChromaDB æœåŠ¡å™¨ç«¯å£
        """
        self.host = host
        self.port = port
        self._client = None
        self._running = False
        self._stop_event = asyncio.Event()
        
    async def _get_client(self):
        """è·å–æˆ–åˆ›å»º ChromaDB å®¢æˆ·ç«¯"""
        if self._client is None:
            try:
                self._client = await chromadb.AsyncHttpClient(
                    host=self.host, 
                    port=self.port
                )
                logger.info(f"å·²è¿æ¥åˆ° ChromaDB: {self.host}:{self.port}")
            except Exception as e:
                logger.error(f"è¿æ¥ ChromaDB å¤±è´¥: {e}")
                raise
        return self._client
    
    async def list_collections(self) -> List[str]:
        """
        åˆ—å‡ºæ‰€æœ‰é›†åˆ
        
        Returns:
            é›†åˆåç§°åˆ—è¡¨
        """
        try:
            client = await self._get_client()
            collections = await client.list_collections()
            collection_names = [col.name for col in collections]
            logger.info(f"å‘ç° {len(collection_names)} ä¸ªé›†åˆ: {collection_names}")
            return collection_names
            
        except Exception as e:
            logger.error(f"åˆ—å‡ºé›†åˆå¤±è´¥: {e}")
            raise
    
    async def get_collection_info(self, collection_name: str) -> Dict[str, Any]:
        """
        è·å–é›†åˆä¿¡æ¯
        
        Args:
            collection_name: é›†åˆåç§°
            
        Returns:
            é›†åˆä¿¡æ¯
        """
        try:
            client = await self._get_client()
            collection = await client.get_collection(collection_name)
            count = await collection.count()
            
            return {
                "name": collection_name,
                "count": count
            }
            
        except Exception as e:
            logger.error(f"è·å–é›†åˆ {collection_name} ä¿¡æ¯å¤±è´¥: {e}")
            raise
    
    async def export_collection_vectors(
        self, 
        collection_name: str, 
        batch_size: int = 1000
    ) -> List[Dict[str, Any]]:
        """
        å¯¼å‡ºé›†åˆä¸­çš„æ‰€æœ‰å‘é‡æ•°æ®
        
        Args:
            collection_name: é›†åˆåç§°
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            å‘é‡æ•°æ®åˆ—è¡¨ï¼ŒåŒ…å«å®Œæ•´çš„æ–‡æ¡£ä¿¡æ¯ï¼ˆé™¤å‘é‡åµŒå…¥å¤–ï¼‰
        """
        try:
            client = await self._get_client()
            collection = await client.get_collection(collection_name)
            
            logger.info(f"å¼€å§‹å¯¼å‡ºé›†åˆ {collection_name} çš„å‘é‡æ•°æ®...")
            
            # è·å–é›†åˆä¸­çš„æ–‡æ¡£æ€»æ•°
            total_count = await collection.count()
            logger.info(f"é›†åˆ {collection_name} å…±æœ‰ {total_count} ä¸ªæ–‡æ¡£")
            
            if total_count == 0:
                logger.warning(f"é›†åˆ {collection_name} ä¸ºç©º")
                return []
            
            all_vectors = []
            
            # åˆ†æ‰¹è·å–æ•°æ®ä»¥é¿å…å†…å­˜é—®é¢˜
            offset = 0
            while offset < total_count:
                try:
                    # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„å¤§å°
                    current_batch_size = min(batch_size, total_count - offset)
                    
                    logger.info(f"æ­£åœ¨è·å–ç¬¬ {offset + 1}-{offset + current_batch_size} ä¸ªæ–‡æ¡£...")
                    
                    # è·å–å½“å‰æ‰¹æ¬¡çš„æ•°æ®
                    results = await collection.get(
                        limit=current_batch_size,
                        offset=offset,
                        include=["documents", "metadatas"]
                    )
                    
                    if not results or not results.get('ids'):
                        logger.warning(f"æ‰¹æ¬¡ {offset}-{offset + current_batch_size} æ²¡æœ‰è¿”å›æ•°æ®")
                        break
                    
                    # å¤„ç†å½“å‰æ‰¹æ¬¡çš„æ•°æ®
                    ids = results.get('ids', [])
                    documents = results.get('documents', [])
                    metadatas = results.get('metadatas', [])
                    
                    batch_vectors = []
                    for i, doc_id in enumerate(ids):
                        # è·å–æ–‡æ¡£å†…å®¹
                        content = documents[i] if i < len(documents) else ""
                        
                        # è·å–å…ƒæ•°æ®
                        metadata = metadatas[i] if i < len(metadatas) and metadatas[i] else {}
                        
                        # æå–æ ‡é¢˜ä¿¡æ¯ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
                        title = "æœªçŸ¥æ ‡é¢˜"
                        if metadata:
                            # å°è¯•å¤šç§å¯èƒ½çš„æ ‡é¢˜å­—æ®µå
                            title_fields = ['title', 'name', 'filename', 'subject', 'topic']
                            for field in title_fields:
                                if field in metadata and metadata[field]:
                                    title = metadata[field]
                                    break
                        
                        # å¦‚æœ metadata ä¸­æ²¡æœ‰æ ‡é¢˜ï¼Œå°è¯•ä»æ–‡æ¡£å†…å®¹ä¸­æå–
                        if title == "æœªçŸ¥æ ‡é¢˜" and content:
                            # å–æ–‡æ¡£å†…å®¹çš„å‰50ä¸ªå­—ç¬¦ä½œä¸ºæ ‡é¢˜
                            if content.strip():
                                title = content.strip()[:50]
                                if len(content.strip()) > 50:
                                    title += "..."
                        
                        # æ„å»ºå®Œæ•´çš„å‘é‡æ•°æ®
                        vector_data = {
                            "id": doc_id,
                            "title": title,  # ç”¨äºæ˜¾ç¤ºçš„æ ‡é¢˜
                            "content": content,  # å®Œæ•´æ–‡æ¡£å†…å®¹
                            "metadata": metadata,  # å®Œæ•´å…ƒæ•°æ®
                            "content_length": len(content) if content else 0,  # å†…å®¹é•¿åº¦
                            "metadata_keys": list(metadata.keys()) if metadata else []  # å…ƒæ•°æ®å­—æ®µåˆ—è¡¨
                        }
                        batch_vectors.append(vector_data)
                    
                    all_vectors.extend(batch_vectors)
                    logger.info(f"âœ… æˆåŠŸå¤„ç† {len(batch_vectors)} ä¸ªå‘é‡ (æ€»è®¡: {len(all_vectors)}/{total_count})")
                    
                    offset += current_batch_size
                    
                except Exception as e:
                    logger.error(f"å¤„ç†æ‰¹æ¬¡ {offset}-{offset + current_batch_size} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    # ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹æ¬¡
                    offset += batch_size
                    continue
            
            logger.info(f"ğŸ‰ æˆåŠŸå¯¼å‡ºé›†åˆ {collection_name} çš„ {len(all_vectors)} ä¸ªå‘é‡")
            return all_vectors
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºé›†åˆ {collection_name} å‘é‡å¤±è´¥: {e}")
            raise
    
    async def export_all_vectors(
        self, 
        collection_names: Optional[List[str]] = None,
        batch_size: int = 1000
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        å¯¼å‡ºæ‰€æœ‰é›†åˆçš„å‘é‡æ•°æ®
        
        Args:
            collection_names: è¦å¯¼å‡ºçš„é›†åˆåç§°åˆ—è¡¨ï¼Œå¦‚æœä¸º None åˆ™å¯¼å‡ºæ‰€æœ‰é›†åˆ
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            æŒ‰é›†åˆåç§°åˆ†ç»„çš„å‘é‡æ•°æ®
        """
        try:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šé›†åˆï¼Œè·å–æ‰€æœ‰é›†åˆ
            if collection_names is None:
                collection_names = await self.list_collections()
            
            if not collection_names:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é›†åˆ")
                return {}
            
            all_results = {}
            
            for collection_name in collection_names:
                try:
                    logger.info(f"ğŸ“Š å¼€å§‹å¤„ç†é›†åˆ: {collection_name}")
                    
                    # è·å–é›†åˆä¿¡æ¯
                    info = await self.get_collection_info(collection_name)
                    logger.info(f"é›†åˆ {collection_name} åŒ…å« {info['count']} ä¸ªæ–‡æ¡£")
                    
                    # å¯¼å‡ºå‘é‡ä¿¡æ¯
                    vectors = await self.export_collection_vectors(collection_name, batch_size)
                    all_results[collection_name] = vectors
                    
                    logger.info(f"âœ… é›†åˆ {collection_name} å¯¼å‡ºå®Œæˆï¼Œå…± {len(vectors)} ä¸ªå‘é‡")
                    
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†é›†åˆ {collection_name} å¤±è´¥: {e}")
                    all_results[collection_name] = []
                    continue
            
            return all_results
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºæ‰€æœ‰å‘é‡å¤±è´¥: {e}")
            raise
    
    def stop(self):
        """åœæ­¢å®šæ—¶å¯¼å‡º"""
        self._running = False
        self._stop_event.set()
    
    async def scheduled_export(
        self, 
        output_dir: str = "./exports",
        collection_names: Optional[List[str]] = None,
        batch_size: int = 1000,
        interval_minutes: int = 10
    ):
        """
        å®šæ—¶å¯¼å‡ºå‘é‡æ•°æ®
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            collection_names: è¦å¯¼å‡ºçš„é›†åˆåç§°åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            interval_minutes: å¯¼å‡ºé—´éš”ï¼ˆåˆ†é’Ÿï¼‰
        """
        self._running = True
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸš€ å¼€å§‹å®šæ—¶å¯¼å‡ºä»»åŠ¡")
        logger.info(f"   å¯¼å‡ºç›®å½•: {output_path.absolute()}")
        logger.info(f"   å¯¼å‡ºé—´éš”: {interval_minutes} åˆ†é’Ÿ")
        logger.info(f"   ChromaDB: {self.host}:{self.port}")
        
        export_count = 0
        
        while self._running:
            try:
                export_count += 1
                start_time = datetime.now()
                
                logger.info(f"ğŸ“Š å¼€å§‹ç¬¬ {export_count} æ¬¡å¯¼å‡º ({start_time.strftime('%Y-%m-%d %H:%M:%S')})")
                
                # å¯¼å‡ºæ•°æ®
                results = await self.export_all_vectors(
                    collection_names=collection_names,
                    batch_size=batch_size
                )
                
                # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
                timestamp = start_time.strftime("%Y%m%d_%H%M%S")
                filename = f"chromadb_export_{timestamp}.json"
                filepath = output_path / filename
                
                # æ„å»ºå¯¼å‡ºæ•°æ®ç»“æ„
                export_data = {
                    "export_info": {
                        "timestamp": start_time.isoformat(),
                        "export_count": export_count,
                        "chromadb_host": self.host,
                        "chromadb_port": self.port,
                        "total_collections": len(results),
                        "total_vectors": sum(len(vectors) for vectors in results.values())
                    },
                    "collections": results
                }
                
                # å†™å…¥JSONæ–‡ä»¶
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(export_data, f, ensure_ascii=False, indent=2)
                
                end_time = datetime.now()
                duration = (end_time - start_time).total_seconds()
                
                logger.info(f"âœ… ç¬¬ {export_count} æ¬¡å¯¼å‡ºå®Œæˆ")
                logger.info(f"   æ–‡ä»¶: {filename}")
                logger.info(f"   å¤§å°: {filepath.stat().st_size / 1024 / 1024:.2f} MB")
                logger.info(f"   è€—æ—¶: {duration:.2f} ç§’")
                logger.info(f"   é›†åˆæ•°: {export_data['export_info']['total_collections']}")
                logger.info(f"   å‘é‡æ•°: {export_data['export_info']['total_vectors']}")
                
                # ç­‰å¾…ä¸‹æ¬¡å¯¼å‡ºæˆ–åœæ­¢ä¿¡å·
                if self._running:
                    logger.info(f"â° ç­‰å¾… {interval_minutes} åˆ†é’Ÿåè¿›è¡Œä¸‹æ¬¡å¯¼å‡º...")
                    try:
                        await asyncio.wait_for(
                            self._stop_event.wait(), 
                            timeout=interval_minutes * 60
                        )
                        # å¦‚æœæ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡ºå¾ªç¯
                        if self._stop_event.is_set():
                            break
                    except asyncio.TimeoutError:
                        # è¶…æ—¶æ˜¯æ­£å¸¸çš„ï¼Œç»§ç»­ä¸‹æ¬¡å¯¼å‡º
                        pass
                
            except Exception as e:
                logger.error(f"âŒ ç¬¬ {export_count} æ¬¡å¯¼å‡ºå¤±è´¥: {e}")
                # å‡ºé”™æ—¶ç­‰å¾…è¾ƒçŸ­æ—¶é—´åé‡è¯•
                if self._running:
                    logger.info("â° ç­‰å¾… 1 åˆ†é’Ÿåé‡è¯•...")
                    try:
                        await asyncio.wait_for(self._stop_event.wait(), timeout=60)
                        if self._stop_event.is_set():
                            break
                    except asyncio.TimeoutError:
                        pass
        
        logger.info(f"ğŸ›‘ å®šæ—¶å¯¼å‡ºä»»åŠ¡å·²åœæ­¢ï¼Œå…±å®Œæˆ {export_count} æ¬¡å¯¼å‡º")
    
    async def close(self):
        """å…³é—­è¿æ¥"""
        self.stop()
        if self._client:
            # ChromaDB å®¢æˆ·ç«¯é€šå¸¸ä¸éœ€è¦æ˜¾å¼å…³é—­
            self._client = None


def format_output(results: Dict[str, List[Dict[str, Any]]], output_format: str = "table") -> str:
    """
    æ ¼å¼åŒ–è¾“å‡ºç»“æœ
    
    Args:
        results: å¯¼å‡ºç»“æœ
        output_format: è¾“å‡ºæ ¼å¼ (table, json, csv, summary)
        
    Returns:
        æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²
    """
    if output_format == "json":
        return json.dumps(results, ensure_ascii=False, indent=2)
    
    elif output_format == "csv":
        lines = ["Collection,ID,Title,Content_Length,Metadata_Keys,Content,Metadata"]
        for collection_name, vectors in results.items():
            for vector in vectors:
                # CSV æ ¼å¼éœ€è¦å¤„ç†ç‰¹æ®Šå­—ç¬¦
                title = vector['title'].replace('"', '""').replace('\n', ' ').replace('\r', ' ')
                content = vector['content'].replace('"', '""').replace('\n', '\\n').replace('\r', '\\r')
                metadata_keys = '|'.join(vector.get('metadata_keys', []))
                metadata_json = json.dumps(vector.get('metadata', {}), ensure_ascii=False).replace('"', '""')
                
                lines.append(f'"{collection_name}","{vector["id"]}","{title}","{vector.get("content_length", 0)}","{metadata_keys}","{content}","{metadata_json}"')
        return '\n'.join(lines)
    
    elif output_format == "summary":
        # æ‘˜è¦æ ¼å¼ï¼šåªæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯å’ŒåŸºæœ¬å­—æ®µ
        output_lines = []
        total_vectors = 0
        
        for collection_name, vectors in results.items():
            output_lines.append(f"\nğŸ“š é›†åˆ: {collection_name}")
            output_lines.append("=" * 100)
            
            if not vectors:
                output_lines.append("   (ç©ºé›†åˆ)")
                continue
            
            output_lines.append(f"{'åºå·':<6} {'ID':<36} {'æ ‡é¢˜':<30} {'å†…å®¹é•¿åº¦':<10} {'å…ƒæ•°æ®å­—æ®µ'}")
            output_lines.append("-" * 100)
            
            for i, vector in enumerate(vectors, 1):
                # é™åˆ¶æ ‡é¢˜é•¿åº¦ä»¥é€‚åº”è¡¨æ ¼æ˜¾ç¤º
                title = vector['title'][:27] + "..." if len(vector['title']) > 27 else vector['title']
                metadata_keys = ', '.join(vector.get('metadata_keys', []))[:20]
                if len(', '.join(vector.get('metadata_keys', []))) > 20:
                    metadata_keys += "..."
                
                output_lines.append(f"{i:<6} {vector['id']:<36} {title:<30} {vector.get('content_length', 0):<10} {metadata_keys}")
            
            output_lines.append(f"\nå°è®¡: {len(vectors)} ä¸ªå‘é‡")
            total_vectors += len(vectors)
        
        output_lines.append(f"\nğŸ‰ æ€»è®¡: {total_vectors} ä¸ªå‘é‡")
        return '\n'.join(output_lines)
    
    else:  # table format - è¯¦ç»†æ ¼å¼
        output_lines = []
        total_vectors = 0
        
        for collection_name, vectors in results.items():
            output_lines.append(f"\nğŸ“š é›†åˆ: {collection_name}")
            output_lines.append("=" * 120)
            
            if not vectors:
                output_lines.append("   (ç©ºé›†åˆ)")
                continue
            
            for i, vector in enumerate(vectors, 1):
                output_lines.append(f"\nğŸ“„ æ–‡æ¡£ #{i}")
                output_lines.append("-" * 60)
                output_lines.append(f"ID: {vector['id']}")
                output_lines.append(f"æ ‡é¢˜: {vector['title']}")
                output_lines.append(f"å†…å®¹é•¿åº¦: {vector.get('content_length', 0)} å­—ç¬¦")
                
                # æ˜¾ç¤ºå…ƒæ•°æ®
                metadata = vector.get('metadata', {})
                if metadata:
                    output_lines.append("å…ƒæ•°æ®:")
                    for key, value in metadata.items():
                        # é™åˆ¶å€¼çš„æ˜¾ç¤ºé•¿åº¦
                        if isinstance(value, str) and len(value) > 100:
                            display_value = value[:100] + "..."
                        else:
                            display_value = str(value)
                        output_lines.append(f"  {key}: {display_value}")
                else:
                    output_lines.append("å…ƒæ•°æ®: (æ— )")
                
                # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
                content = vector.get('content', '')
                if content:
                    preview = content[:200] + "..." if len(content) > 200 else content
                    output_lines.append(f"å†…å®¹é¢„è§ˆ: {preview}")
                else:
                    output_lines.append("å†…å®¹: (ç©º)")
            
            output_lines.append(f"\nå°è®¡: {len(vectors)} ä¸ªå‘é‡")
            total_vectors += len(vectors)
        
        output_lines.append(f"\nğŸ‰ æ€»è®¡: {total_vectors} ä¸ªå‘é‡")
        return '\n'.join(output_lines)


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="ChromaDB å‘é‡å¯¼å‡ºå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä¸€æ¬¡æ€§å¯¼å‡ºæ‰€æœ‰é›†åˆçš„å‘é‡æ•°æ®ï¼ˆæ‘˜è¦æ ¼å¼ï¼‰
  python export_vectors.py
  
  # å®šæ—¶å¯¼å‡ºæ¨¡å¼ï¼šæ¯10åˆ†é’Ÿå¯¼å‡ºä¸€æ¬¡åˆ°JSONæ–‡ä»¶
  python export_vectors.py --scheduled
  
  # è‡ªå®šä¹‰å®šæ—¶å¯¼å‡ºé—´éš”å’Œè¾“å‡ºç›®å½•
  python export_vectors.py --scheduled --interval 5 --output-dir /path/to/exports
  
  # æŒ‡å®š ChromaDB æœåŠ¡å™¨
  python export_vectors.py --host localhost --port 8000
  
  # åªå¯¼å‡ºç‰¹å®šé›†åˆï¼ˆå®šæ—¶æ¨¡å¼ï¼‰
  python export_vectors.py --scheduled --collections math_knowledge
  
  # è¯¦ç»†æ ¼å¼æ˜¾ç¤ºå®Œæ•´å†…å®¹å’Œå…ƒæ•°æ®ï¼ˆä¸€æ¬¡æ€§å¯¼å‡ºï¼‰
  python export_vectors.py --format table
  
  # è¾“å‡ºä¸º JSON æ ¼å¼ï¼ˆåŒ…å«æ‰€æœ‰æ•°æ®ï¼‰
  python export_vectors.py --format json
  
  # è¾“å‡ºä¸º CSV æ ¼å¼
  python export_vectors.py --format csv --output vectors.csv
        """
    )
    
    parser.add_argument(
        "--host",
        default="localhost",
        help="ChromaDB æœåŠ¡å™¨åœ°å€ (é»˜è®¤: localhost)"
    )
    
    parser.add_argument(
        "--port",
        type=int,
        default=8000,
        help="ChromaDB æœåŠ¡å™¨ç«¯å£ (é»˜è®¤: 8000)"
    )
    
    parser.add_argument(
        "--collections",
        nargs="+",
        help="è¦å¯¼å‡ºçš„é›†åˆåç§°åˆ—è¡¨ (é»˜è®¤: æ‰€æœ‰é›†åˆ)"
    )
    
    parser.add_argument(
        "--format",
        choices=["table", "summary", "json", "csv"],
        default="summary",
        help="è¾“å‡ºæ ¼å¼: table(è¯¦ç»†), summary(æ‘˜è¦), json(JSON), csv(CSV) (é»˜è®¤: summary)"
    )
    
    parser.add_argument(
        "--output",
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤: è¾“å‡ºåˆ°æ§åˆ¶å°)"
    )
    
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1000,
        help="æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 1000)"
    )
    
    parser.add_argument(
        "--quiet",
        action="store_true",
        help="é™é»˜æ¨¡å¼ï¼Œåªè¾“å‡ºç»“æœ"
    )
    
    parser.add_argument(
        "--scheduled",
        action="store_true",
        help="å¯ç”¨å®šæ—¶å¯¼å‡ºæ¨¡å¼ï¼Œæ¯10åˆ†é’Ÿå¯¼å‡ºä¸€æ¬¡åˆ°JSONæ–‡ä»¶"
    )
    
    parser.add_argument(
        "--output-dir",
        default="./exports",
        help="å®šæ—¶å¯¼å‡ºæ¨¡å¼çš„è¾“å‡ºç›®å½• (é»˜è®¤: ./exports)"
    )
    
    parser.add_argument(
        "--interval",
        type=int,
        default=10,
        help="å®šæ—¶å¯¼å‡ºé—´éš”ï¼ˆåˆ†é’Ÿï¼‰ (é»˜è®¤: 10)"
    )
    
    return parser.parse_args()


async def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    # é…ç½®æ—¥å¿—
    if args.quiet:
        logger.remove()
        logger.add(sys.stderr, level="ERROR")
    else:
        logger.remove()
        logger.add(sys.stderr, level="INFO", format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | {message}")
    
    if not args.quiet:
        print("ğŸš€ ChromaDB å‘é‡å¯¼å‡ºå·¥å…·")
        print("=" * 50)
        print(f"æœåŠ¡å™¨: {args.host}:{args.port}")
        print(f"é›†åˆ: {args.collections if args.collections else 'æ‰€æœ‰é›†åˆ'}")
        if args.scheduled:
            print(f"æ¨¡å¼: å®šæ—¶å¯¼å‡º (æ¯ {args.interval} åˆ†é’Ÿ)")
            print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
        else:
            print(f"æ¨¡å¼: ä¸€æ¬¡æ€§å¯¼å‡º")
            print(f"è¾“å‡ºæ ¼å¼: {args.format}")
        print(f"æ‰¹å¤„ç†å¤§å°: {args.batch_size}")
        print("")
    
    # åˆ›å»ºå¯¼å‡ºå™¨
    exporter = VectorExporter(host=args.host, port=args.port)
    
    # è®¾ç½®ä¿¡å·å¤„ç†å™¨ç”¨äºä¼˜é›…é€€å‡º
    def signal_handler(signum, frame):
        logger.info("æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨åœæ­¢...")
        exporter.stop()
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        if args.scheduled:
            # å®šæ—¶å¯¼å‡ºæ¨¡å¼
            if not args.quiet:
                print("ğŸ”„ å¯åŠ¨å®šæ—¶å¯¼å‡ºæ¨¡å¼...")
                print("æŒ‰ Ctrl+C åœæ­¢å¯¼å‡º")
                print("")
            
            await exporter.scheduled_export(
                output_dir=args.output_dir,
                collection_names=args.collections,
                batch_size=args.batch_size,
                interval_minutes=args.interval
            )
        else:
            # ä¸€æ¬¡æ€§å¯¼å‡ºæ¨¡å¼
            try:
                # å¯¼å‡ºå‘é‡æ•°æ®
                results = await exporter.export_all_vectors(
                    collection_names=args.collections,
                    batch_size=args.batch_size
                )
                
                # æ ¼å¼åŒ–è¾“å‡º
                output_text = format_output(results, args.format)
                
                # è¾“å‡ºç»“æœ
                if args.output:
                    with open(args.output, 'w', encoding='utf-8') as f:
                        f.write(output_text)
                    if not args.quiet:
                        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
                else:
                    print(output_text)
                
                if not args.quiet:
                    # ç»Ÿè®¡ä¿¡æ¯
                    total_collections = len(results)
                    total_vectors = sum(len(vectors) for vectors in results.values())
                    print(f"\nğŸ“Š å¯¼å‡ºç»Ÿè®¡:")
                    print(f"   é›†åˆæ•°é‡: {total_collections}")
                    print(f"   å‘é‡æ€»æ•°: {total_vectors}")
                    print(f"   å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            
            finally:
                await exporter.close()
            
            if not args.quiet:
                print("\nğŸ‰ å¯¼å‡ºå®Œæˆ!")
            
    except KeyboardInterrupt:
        if args.scheduled:
            print("\nâ¹ï¸  å®šæ—¶å¯¼å‡ºå·²åœæ­¢")
        else:
            print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­äº†å¯¼å‡ºè¿‡ç¨‹")
    except Exception as e:
        logger.error(f"ğŸ’¥ å¯¼å‡ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        if not args.quiet:
            import traceback
            traceback.print_exc()
        sys.exit(1)
    finally:
        await exporter.close()


if __name__ == "__main__":
    asyncio.run(main())
