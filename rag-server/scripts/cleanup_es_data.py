#!/usr/bin/env python3
"""
Elasticsearch æ•°æ®æ¸…ç†è„šæœ¬
ä» ChromaDB ä¸­è·å–æ‰€æœ‰æ–‡æ¡£ IDï¼Œç„¶ååˆ é™¤ Elasticsearch ä¸­ä¸åœ¨æ­¤åˆ—è¡¨ä¸­çš„æ–‡æ¡£
åˆ é™¤å‰ä¼šè¦æ±‚ç”¨æˆ·ç¡®è®¤ï¼Œç¡®ä¿æ•°æ®å®‰å…¨
"""
import asyncio
import sys
import os
import argparse
import json
from typing import List, Dict, Any, Set, Optional
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import chromadb
from elasticsearch import AsyncElasticsearch
from loguru import logger


class DataCleanupService:
    """æ•°æ®æ¸…ç†æœåŠ¡"""
    
    def __init__(
        self, 
        chroma_host: str = "localhost", 
        chroma_port: int = 8000,
        es_host: str = "localhost",
        es_port: int = 9200,
        es_index: str = "math_knowledge"
    ):
        """
        åˆå§‹åŒ–æ•°æ®æ¸…ç†æœåŠ¡
        
        Args:
            chroma_host: ChromaDB æœåŠ¡å™¨åœ°å€
            chroma_port: ChromaDB æœåŠ¡å™¨ç«¯å£
            es_host: Elasticsearch æœåŠ¡å™¨åœ°å€
            es_port: Elasticsearch æœåŠ¡å™¨ç«¯å£
            es_index: Elasticsearch ç´¢å¼•åç§°
        """
        self.chroma_host = chroma_host
        self.chroma_port = chroma_port
        self.es_host = es_host
        self.es_port = es_port
        self.es_index = es_index
        
        self._chroma_client = None
        self._es_client = None
    
    async def _get_chroma_client(self):
        """è·å–æˆ–åˆ›å»º ChromaDB å®¢æˆ·ç«¯"""
        if self._chroma_client is None:
            try:
                self._chroma_client = await chromadb.AsyncHttpClient(
                    host=self.chroma_host, 
                    port=self.chroma_port
                )
                logger.info(f"å·²è¿æ¥åˆ° ChromaDB: {self.chroma_host}:{self.chroma_port}")
            except Exception as e:
                logger.error(f"è¿æ¥ ChromaDB å¤±è´¥: {e}")
                raise
        return self._chroma_client
    
    async def _get_es_client(self):
        """è·å–æˆ–åˆ›å»º Elasticsearch å®¢æˆ·ç«¯"""
        if self._es_client is None:
            try:
                self._es_client = AsyncElasticsearch(
                    hosts=[{"host": self.es_host, "port": self.es_port, "scheme": "http"}],
                    verify_certs=False,
                    ssl_show_warn=False,
                    request_timeout=30,
                    retry_on_timeout=True,
                    max_retries=3
                )
                # æµ‹è¯•è¿æ¥
                info = await self._es_client.info()
                logger.info(f"å·²è¿æ¥åˆ° Elasticsearch: {self.es_host}:{self.es_port}")
                logger.info(f"ES ç‰ˆæœ¬: {info.get('version', {}).get('number', 'unknown')}")
            except Exception as e:
                logger.error(f"è¿æ¥ Elasticsearch å¤±è´¥: {e}")
                raise
        return self._es_client
    
    async def get_chromadb_collections(self) -> List[str]:
        """
        è·å– ChromaDB ä¸­çš„æ‰€æœ‰é›†åˆåç§°
        
        Returns:
            é›†åˆåç§°åˆ—è¡¨
        """
        try:
            client = await self._get_chroma_client()
            collections = await client.list_collections()
            collection_names = [col.name for col in collections]
            logger.info(f"å‘ç° ChromaDB é›†åˆ: {collection_names}")
            return collection_names
        except Exception as e:
            logger.error(f"è·å– ChromaDB é›†åˆå¤±è´¥: {e}")
            raise
    
    async def get_all_chromadb_ids(
        self, 
        collection_names: Optional[List[str]] = None,
        batch_size: int = 1000
    ) -> Set[str]:
        """
        è·å– ChromaDB ä¸­æ‰€æœ‰æ–‡æ¡£çš„ ID
        
        Args:
            collection_names: è¦æ£€æŸ¥çš„é›†åˆåç§°åˆ—è¡¨ï¼Œå¦‚æœä¸º None åˆ™æ£€æŸ¥æ‰€æœ‰é›†åˆ
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            æ‰€æœ‰æ–‡æ¡£ ID çš„é›†åˆ
        """
        try:
            client = await self._get_chroma_client()
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®šé›†åˆï¼Œè·å–æ‰€æœ‰é›†åˆ
            if collection_names is None:
                collection_names = await self.get_chromadb_collections()
            
            all_ids = set()
            
            for collection_name in collection_names:
                try:
                    logger.info(f"æ­£åœ¨è·å–é›†åˆ {collection_name} çš„æ–‡æ¡£ ID...")
                    
                    collection = await client.get_collection(collection_name)
                    total_count = await collection.count()
                    
                    logger.info(f"é›†åˆ {collection_name} å…±æœ‰ {total_count} ä¸ªæ–‡æ¡£")
                    
                    if total_count == 0:
                        continue
                    
                    # åˆ†æ‰¹è·å– ID
                    offset = 0
                    while offset < total_count:
                        current_batch_size = min(batch_size, total_count - offset)
                        
                        results = await collection.get(
                            limit=current_batch_size,
                            offset=offset,
                            include=[]  # åªè·å– IDï¼Œä¸éœ€è¦å…¶ä»–æ•°æ®
                        )
                        
                        if not results or not results.get('ids'):
                            break
                        
                        batch_ids = results['ids']
                        all_ids.update(batch_ids)
                        
                        logger.info(f"å·²è·å– {len(batch_ids)} ä¸ª ID (æ€»è®¡: {len(all_ids)})")
                        offset += current_batch_size
                    
                    logger.info(f"âœ… é›†åˆ {collection_name} å®Œæˆï¼Œè·å– {len(all_ids)} ä¸ª ID")
                    
                except Exception as e:
                    logger.error(f"è·å–é›†åˆ {collection_name} çš„ ID å¤±è´¥: {e}")
                    continue
            
            logger.info(f"ğŸ‰ ChromaDB æ€»å…±æœ‰ {len(all_ids)} ä¸ªå”¯ä¸€æ–‡æ¡£ ID")
            return all_ids
            
        except Exception as e:
            logger.error(f"è·å– ChromaDB æ–‡æ¡£ ID å¤±è´¥: {e}")
            raise
    
    async def get_all_es_ids(self, batch_size: int = 1000) -> Set[str]:
        """
        è·å– Elasticsearch ä¸­æ‰€æœ‰æ–‡æ¡£çš„ ID
        
        Args:
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            æ‰€æœ‰æ–‡æ¡£ ID çš„é›†åˆ
        """
        try:
            es_client = await self._get_es_client()
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            try:
                exists_response = await es_client.indices.exists(index=self.es_index)
                if not exists_response:
                    logger.warning(f"Elasticsearch ç´¢å¼• {self.es_index} ä¸å­˜åœ¨")
                    return set()
            except Exception as e:
                logger.error(f"æ£€æŸ¥ç´¢å¼•å­˜åœ¨æ€§å¤±è´¥: {e}")
                # å°è¯•ç›´æ¥æœç´¢ï¼Œå¦‚æœç´¢å¼•ä¸å­˜åœ¨ä¼šè¿”å›é”™è¯¯
                pass
            
            logger.info(f"æ­£åœ¨è·å– Elasticsearch ç´¢å¼• {self.es_index} çš„æ‰€æœ‰æ–‡æ¡£ ID...")
            
            all_ids = set()
            
            # ä½¿ç”¨ scroll API è·å–æ‰€æœ‰æ–‡æ¡£ ID
            search_body = {
                "query": {"match_all": {}},
                "_source": False,  # ä¸éœ€è¦æ–‡æ¡£å†…å®¹ï¼Œåªè¦ ID
                "size": batch_size
            }
            
            try:
                response = await es_client.search(
                    index=self.es_index,
                    body=search_body,
                    scroll="5m"
                )
            except Exception as e:
                if "index_not_found_exception" in str(e).lower():
                    logger.warning(f"Elasticsearch ç´¢å¼• {self.es_index} ä¸å­˜åœ¨")
                    return set()
                else:
                    raise
            
            scroll_id = response.get("_scroll_id")
            hits = response["hits"]["hits"]
            
            while hits:
                # æå–å½“å‰æ‰¹æ¬¡çš„ ID
                batch_ids = [hit["_id"] for hit in hits]
                all_ids.update(batch_ids)
                
                logger.info(f"å·²è·å– {len(batch_ids)} ä¸ª ID (æ€»è®¡: {len(all_ids)})")
                
                # è·å–ä¸‹ä¸€æ‰¹æ•°æ®
                if scroll_id:
                    response = await es_client.scroll(
                        scroll_id=scroll_id,
                        scroll="5m"
                    )
                    hits = response["hits"]["hits"]
                else:
                    break
            
            # æ¸…ç† scroll
            if scroll_id:
                await es_client.clear_scroll(scroll_id=scroll_id)
            
            logger.info(f"ğŸ‰ Elasticsearch æ€»å…±æœ‰ {len(all_ids)} ä¸ªæ–‡æ¡£ ID")
            return all_ids
            
        except Exception as e:
            logger.error(f"è·å– Elasticsearch æ–‡æ¡£ ID å¤±è´¥: {e}")
            raise
    
    async def find_orphaned_es_documents(
        self, 
        collection_names: Optional[List[str]] = None,
        batch_size: int = 1000
    ) -> Set[str]:
        """
        æ‰¾å‡º Elasticsearch ä¸­å­˜åœ¨ä½† ChromaDB ä¸­ä¸å­˜åœ¨çš„æ–‡æ¡£ ID
        
        Args:
            collection_names: è¦æ£€æŸ¥çš„ ChromaDB é›†åˆåç§°åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            éœ€è¦åˆ é™¤çš„æ–‡æ¡£ ID é›†åˆ
        """
        try:
            logger.info("ğŸ” å¼€å§‹æŸ¥æ‰¾å­¤ç«‹çš„ Elasticsearch æ–‡æ¡£...")
            
            # è·å– ChromaDB ä¸­çš„æ‰€æœ‰ ID
            chroma_ids = await self.get_all_chromadb_ids(collection_names, batch_size)
            
            # è·å– Elasticsearch ä¸­çš„æ‰€æœ‰ ID
            es_ids = await self.get_all_es_ids(batch_size)
            
            # æ‰¾å‡ºå·®é›†ï¼šåœ¨ ES ä¸­ä½†ä¸åœ¨ ChromaDB ä¸­çš„ ID
            orphaned_ids = es_ids - chroma_ids
            
            logger.info(f"ğŸ“Š æ•°æ®å¯¹æ¯”ç»“æœ:")
            logger.info(f"   ChromaDB æ–‡æ¡£æ•°: {len(chroma_ids)}")
            logger.info(f"   Elasticsearch æ–‡æ¡£æ•°: {len(es_ids)}")
            logger.info(f"   å­¤ç«‹æ–‡æ¡£æ•°: {len(orphaned_ids)}")
            
            return orphaned_ids
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾å­¤ç«‹æ–‡æ¡£å¤±è´¥: {e}")
            raise
    
    async def delete_es_documents(
        self, 
        document_ids: Set[str], 
        batch_size: int = 100,
        dry_run: bool = False
    ) -> Dict[str, Any]:
        """
        åˆ é™¤ Elasticsearch ä¸­çš„æŒ‡å®šæ–‡æ¡£
        
        Args:
            document_ids: è¦åˆ é™¤çš„æ–‡æ¡£ ID é›†åˆ
            batch_size: æ‰¹å¤„ç†å¤§å°
            dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œæ¨¡å¼
            
        Returns:
            åˆ é™¤ç»“æœç»Ÿè®¡
        """
        if not document_ids:
            logger.info("æ²¡æœ‰éœ€è¦åˆ é™¤çš„æ–‡æ¡£")
            return {"deleted": 0, "failed": 0, "errors": []}
        
        try:
            es_client = await self._get_es_client()
            
            total_docs = len(document_ids)
            deleted_count = 0
            failed_count = 0
            errors = []
            
            if dry_run:
                logger.info(f"ğŸ§ª è¯•è¿è¡Œæ¨¡å¼ï¼šå°†åˆ é™¤ {total_docs} ä¸ªæ–‡æ¡£")
                return {"deleted": total_docs, "failed": 0, "errors": []}
            
            logger.info(f"ğŸ—‘ï¸  å¼€å§‹åˆ é™¤ {total_docs} ä¸ªæ–‡æ¡£...")
            
            # å°† ID åˆ—è¡¨è½¬æ¢ä¸ºåˆ—è¡¨å¹¶åˆ†æ‰¹å¤„ç†
            id_list = list(document_ids)
            
            for i in range(0, len(id_list), batch_size):
                batch_ids = id_list[i:i + batch_size]
                batch_num = i // batch_size + 1
                total_batches = (len(id_list) + batch_size - 1) // batch_size
                
                logger.info(f"æ­£åœ¨å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹ ({len(batch_ids)} ä¸ªæ–‡æ¡£)...")
                
                try:
                    # æ„å»ºæ‰¹é‡åˆ é™¤è¯·æ±‚
                    bulk_body = []
                    for doc_id in batch_ids:
                        bulk_body.append({
                            "delete": {
                                "_index": self.es_index,
                                "_id": doc_id
                            }
                        })
                    
                    # æ‰§è¡Œæ‰¹é‡åˆ é™¤
                    response = await es_client.bulk(body=bulk_body)
                    
                    # å¤„ç†å“åº”
                    if response.get("errors"):
                        for item in response["items"]:
                            delete_result = item.get("delete", {})
                            if delete_result.get("status") in [200, 404]:  # 200=åˆ é™¤æˆåŠŸ, 404=æ–‡æ¡£ä¸å­˜åœ¨
                                deleted_count += 1
                            else:
                                failed_count += 1
                                error_msg = delete_result.get("error", {}).get("reason", "æœªçŸ¥é”™è¯¯")
                                errors.append(f"åˆ é™¤ {delete_result.get('_id')} å¤±è´¥: {error_msg}")
                    else:
                        # æ²¡æœ‰é”™è¯¯ï¼Œæ‰€æœ‰æ–‡æ¡£éƒ½åˆ é™¤æˆåŠŸ
                        deleted_count += len(batch_ids)
                    
                    logger.info(f"âœ… ç¬¬ {batch_num} æ‰¹å®Œæˆ (æˆåŠŸ: {deleted_count}, å¤±è´¥: {failed_count})")
                    
                except Exception as e:
                    logger.error(f"âŒ ç¬¬ {batch_num} æ‰¹åˆ é™¤å¤±è´¥: {e}")
                    failed_count += len(batch_ids)
                    errors.append(f"æ‰¹æ¬¡ {batch_num} æ•´ä½“å¤±è´¥: {str(e)}")
            
            result = {
                "deleted": deleted_count,
                "failed": failed_count,
                "errors": errors
            }
            
            logger.info(f"ğŸ‰ åˆ é™¤å®Œæˆ:")
            logger.info(f"   æˆåŠŸåˆ é™¤: {deleted_count}")
            logger.info(f"   åˆ é™¤å¤±è´¥: {failed_count}")
            
            if errors:
                logger.warning(f"   é”™è¯¯æ•°é‡: {len(errors)}")
                for error in errors[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                    logger.warning(f"     {error}")
                if len(errors) > 5:
                    logger.warning(f"     ... è¿˜æœ‰ {len(errors) - 5} ä¸ªé”™è¯¯")
            
            return result
            
        except Exception as e:
            logger.error(f"åˆ é™¤ Elasticsearch æ–‡æ¡£å¤±è´¥: {e}")
            raise
    
    async def cleanup_orphaned_documents(
        self,
        collection_names: Optional[List[str]] = None,
        batch_size: int = 1000,
        delete_batch_size: int = 100,
        dry_run: bool = False,
        auto_confirm: bool = False
    ) -> Dict[str, Any]:
        """
        æ¸…ç†å­¤ç«‹çš„ Elasticsearch æ–‡æ¡£
        
        Args:
            collection_names: è¦æ£€æŸ¥çš„ ChromaDB é›†åˆåç§°åˆ—è¡¨
            batch_size: è·å–æ•°æ®çš„æ‰¹å¤„ç†å¤§å°
            delete_batch_size: åˆ é™¤æ“ä½œçš„æ‰¹å¤„ç†å¤§å°
            dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œæ¨¡å¼
            auto_confirm: æ˜¯å¦è‡ªåŠ¨ç¡®è®¤åˆ é™¤
            
        Returns:
            æ¸…ç†ç»“æœç»Ÿè®¡
        """
        try:
            # 1. æŸ¥æ‰¾å­¤ç«‹æ–‡æ¡£
            orphaned_ids = await self.find_orphaned_es_documents(collection_names, batch_size)
            
            if not orphaned_ids:
                logger.info("âœ… æ²¡æœ‰å‘ç°å­¤ç«‹çš„æ–‡æ¡£ï¼Œæ•°æ®å·²åŒæ­¥")
                return {"deleted": 0, "failed": 0, "errors": []}
            
            # 2. æ˜¾ç¤ºè¦åˆ é™¤çš„æ–‡æ¡£ä¿¡æ¯
            logger.warning(f"âš ï¸  å‘ç° {len(orphaned_ids)} ä¸ªå­¤ç«‹æ–‡æ¡£éœ€è¦åˆ é™¤")
            
            # æ˜¾ç¤ºéƒ¨åˆ† ID ä½œä¸ºç¤ºä¾‹
            sample_ids = list(orphaned_ids)[:10]
            logger.info("ç¤ºä¾‹æ–‡æ¡£ ID:")
            for i, doc_id in enumerate(sample_ids, 1):
                logger.info(f"  {i}. {doc_id}")
            if len(orphaned_ids) > 10:
                logger.info(f"  ... è¿˜æœ‰ {len(orphaned_ids) - 10} ä¸ªæ–‡æ¡£")
            
            # 3. ç¡®è®¤åˆ é™¤
            if not dry_run and not auto_confirm:
                print(f"\nâš ï¸  è­¦å‘Šï¼šå³å°†åˆ é™¤ {len(orphaned_ids)} ä¸ª Elasticsearch æ–‡æ¡£ï¼")
                print("è¿™äº›æ–‡æ¡£åœ¨ ChromaDB ä¸­ä¸å­˜åœ¨ï¼Œå¯èƒ½æ˜¯æ•°æ®ä¸åŒæ­¥å¯¼è‡´çš„ã€‚")
                print("åˆ é™¤åæ— æ³•æ¢å¤ï¼Œè¯·ç¡®ä¿è¿™æ˜¯æ‚¨æƒ³è¦çš„æ“ä½œã€‚")
                
                while True:
                    confirm = input("\næ˜¯å¦ç»§ç»­åˆ é™¤ï¼Ÿ(yes/no): ").strip().lower()
                    if confirm in ['yes', 'y']:
                        break
                    elif confirm in ['no', 'n']:
                        logger.info("ç”¨æˆ·å–æ¶ˆäº†åˆ é™¤æ“ä½œ")
                        return {"deleted": 0, "failed": 0, "errors": [], "cancelled": True}
                    else:
                        print("è¯·è¾“å…¥ 'yes' æˆ– 'no'")
            
            # 4. æ‰§è¡Œåˆ é™¤
            result = await self.delete_es_documents(
                orphaned_ids, 
                delete_batch_size, 
                dry_run
            )
            
            return result
            
        except Exception as e:
            logger.error(f"æ¸…ç†å­¤ç«‹æ–‡æ¡£å¤±è´¥: {e}")
            raise
    
    async def close(self):
        """å…³é—­è¿æ¥"""
        if self._chroma_client:
            self._chroma_client = None
        
        if self._es_client:
            await self._es_client.close()
            self._es_client = None


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="Elasticsearch æ•°æ®æ¸…ç†å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # æŸ¥æ‰¾å¹¶åˆ é™¤å­¤ç«‹çš„ ES æ–‡æ¡£ï¼ˆéœ€è¦ç¡®è®¤ï¼‰
  python cleanup_es_data.py
  
  # è¯•è¿è¡Œæ¨¡å¼ï¼ŒåªæŸ¥çœ‹ä¸åˆ é™¤
  python cleanup_es_data.py --dry-run
  
  # è‡ªåŠ¨ç¡®è®¤åˆ é™¤ï¼Œä¸éœ€è¦æ‰‹åŠ¨ç¡®è®¤
  python cleanup_es_data.py --auto-confirm
  
  # æŒ‡å®šæœåŠ¡å™¨åœ°å€
  python cleanup_es_data.py --chroma-host localhost --es-host localhost
  
  # åªæ£€æŸ¥ç‰¹å®šçš„ ChromaDB é›†åˆ
  python cleanup_es_data.py --collections math_knowledge
  
  # æŒ‡å®š ES ç´¢å¼•
  python cleanup_es_data.py --es-index my_knowledge_base
        """
    )
    
    parser.add_argument(
        "--chroma-host",
        default="localhost",
        help="ChromaDB æœåŠ¡å™¨åœ°å€ (é»˜è®¤: localhost)"
    )
    
    parser.add_argument(
        "--chroma-port",
        type=int,
        default=8000,
        help="ChromaDB æœåŠ¡å™¨ç«¯å£ (é»˜è®¤: 8000)"
    )
    
    parser.add_argument(
        "--es-host",
        default="localhost",
        help="Elasticsearch æœåŠ¡å™¨åœ°å€ (é»˜è®¤: localhost)"
    )
    
    parser.add_argument(
        "--es-port",
        type=int,
        default=9200,
        help="Elasticsearch æœåŠ¡å™¨ç«¯å£ (é»˜è®¤: 9200)"
    )
    
    parser.add_argument(
        "--es-index",
        default="math_knowledge",
        help="Elasticsearch ç´¢å¼•åç§° (é»˜è®¤: math_knowledge)"
    )
    
    parser.add_argument(
        "--collections",
        nargs="+",
        help="è¦æ£€æŸ¥çš„ ChromaDB é›†åˆåç§°åˆ—è¡¨ (é»˜è®¤: æ‰€æœ‰é›†åˆ)"
    )
    
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1000,
        help="è·å–æ•°æ®çš„æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 1000)"
    )
    
    parser.add_argument(
        "--delete-batch-size",
        type=int,
        default=100,
        help="åˆ é™¤æ“ä½œçš„æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 100)"
    )
    
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="è¯•è¿è¡Œæ¨¡å¼ï¼ŒåªæŸ¥çœ‹ä¸åˆ é™¤"
    )
    
    parser.add_argument(
        "--auto-confirm",
        action="store_true",
        help="è‡ªåŠ¨ç¡®è®¤åˆ é™¤ï¼Œä¸éœ€è¦æ‰‹åŠ¨ç¡®è®¤"
    )
    
    parser.add_argument(
        "--quiet",
        action="store_true",
        help="é™é»˜æ¨¡å¼ï¼Œå‡å°‘è¾“å‡º"
    )
    
    return parser.parse_args()


async def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    # é…ç½®æ—¥å¿—
    if args.quiet:
        logger.remove()
        logger.add(sys.stderr, level="WARNING")
    else:
        logger.remove()
        logger.add(sys.stderr, level="INFO", format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | {message}")
    
    if not args.quiet:
        print("ğŸ§¹ Elasticsearch æ•°æ®æ¸…ç†å·¥å…·")
        print("=" * 50)
        print(f"ChromaDB: {args.chroma_host}:{args.chroma_port}")
        print(f"Elasticsearch: {args.es_host}:{args.es_port}")
        print(f"ES ç´¢å¼•: {args.es_index}")
        print(f"é›†åˆ: {args.collections if args.collections else 'æ‰€æœ‰é›†åˆ'}")
        print(f"æ¨¡å¼: {'è¯•è¿è¡Œ' if args.dry_run else 'æ­£å¼æ¸…ç†'}")
        print(f"æ‰¹å¤„ç†å¤§å°: {args.batch_size}")
        print("")
    
    try:
        # åˆ›å»ºæ¸…ç†æœåŠ¡
        cleanup_service = DataCleanupService(
            chroma_host=args.chroma_host,
            chroma_port=args.chroma_port,
            es_host=args.es_host,
            es_port=args.es_port,
            es_index=args.es_index
        )
        
        try:
            # æ‰§è¡Œæ¸…ç†
            result = await cleanup_service.cleanup_orphaned_documents(
                collection_names=args.collections,
                batch_size=args.batch_size,
                delete_batch_size=args.delete_batch_size,
                dry_run=args.dry_run,
                auto_confirm=args.auto_confirm
            )
            
            if not args.quiet:
                print(f"\nğŸ“Š æ¸…ç†ç»“æœ:")
                if result.get("cancelled"):
                    print("   æ“ä½œå·²å–æ¶ˆ")
                else:
                    print(f"   åˆ é™¤æˆåŠŸ: {result.get('deleted', 0)}")
                    print(f"   åˆ é™¤å¤±è´¥: {result.get('failed', 0)}")
                    if result.get('errors'):
                        print(f"   é”™è¯¯æ•°é‡: {len(result['errors'])}")
                print(f"   å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        finally:
            await cleanup_service.close()
        
        if not args.quiet:
            if args.dry_run:
                print("\nğŸ§ª è¯•è¿è¡Œå®Œæˆï¼è¦æ‰§è¡Œå®é™…åˆ é™¤ï¼Œè¯·ç§»é™¤ --dry-run å‚æ•°")
            else:
                print("\nğŸ‰ æ¸…ç†å®Œæˆï¼")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­äº†æ¸…ç†è¿‡ç¨‹")
        sys.exit(1)
    except Exception as e:
        logger.error(f"ğŸ’¥ æ¸…ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        if not args.quiet:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
